{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trial.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNUUxj8pC+OlTj0/TbiAA7l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"doUtNr4LG3Gp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":322},"outputId":"960838fb-c0e9-4ee0-99d8-749adb8198ea","executionInfo":{"status":"ok","timestamp":1590804419491,"user_tz":240,"elapsed":7317,"user":{"displayName":"Milan Bidare","photoUrl":"","userId":"17945590210837636468"}}},"source":["!pip install gym\n","!pip install Box2D gym\n","#!pip install torch"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Collecting Box2D\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Installing collected packages: Box2D\n","Successfully installed Box2D-2.3.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BjEDO-yJHRYH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"61e5139f-af04-45bc-f6e1-ac74d0484064"},"source":["import easydict\n","import gym\n","import numpy as np\n","from itertools import count\n","from collections import namedtuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Normal\n","import matplotlib.pyplot as plt\n","# Cart Pole\n","# based on:\n","# https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\n","\n","#args = parser.parse_args()\n","\n","args = easydict.EasyDict({\n","    \"gamma\": 0.99,\n","    \"seed\": 203,\n","    \"render\":False,\n","    \"log_interval\":10\n","})\n","\n","env = gym.make('LunarLanderContinuous-v2')\n","\n","env.seed(args.seed)\n","torch.manual_seed(args.seed)\n","\n","SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","\n","class Policy(nn.Module):\n","    \"\"\"\n","    implements both actor and critic in one model\n","    \"\"\"\n","    def __init__(self, hidden_dim1=64, hidden_dim2=32, output_dim = 128):\n","        super(Policy, self).__init__()\n","        self.affine1 = nn.Linear(state_dim, hidden_dim1) \n","        self.affine2 = nn.Linear(hidden_dim1,hidden_dim2)\n","        self.affine3 = nn.Linear(hidden_dim2,output_dim)\n","        self.act1 = nn.ReLU()\n","        # actor's layer\n","        self.action_mean = nn.Linear(output_dim, action_dim) \n","        self.action_var = nn.Linear(output_dim, action_dim)\n","        # critic's layer\n","        self.value_head = nn.Linear(output_dim, 1)\n","        # action & reward buffer\n","        self.saved_actions = []\n","        self.rewards = []\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        forward of both actor and critic\n","        \"\"\"\n","        # TODO map input to:\n","        # mean of action distribution,\n","        # variance of action distribution (pass this through a non-negative function),\n","        # state value\n","        \n","        x = self.act1(self.affine1(x))\n","        x = self.act1(self.affine2(x))\n","        x = self.act1(self.affine3(x))\n","        action_mean = self.action_mean(x)\n","        action_var = F.softplus(self.action_var(x))\n","        state_values = self.value_head(x) # <= Value Function not value of state\n","        \n","        return 0.5*action_mean, 0.5*action_var, state_values\n","    \n","model = Policy().float()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","eps = np.finfo(np.float32).eps.item()\n","\n","def select_action(state):\n","    state = torch.from_numpy(state).float()\n","    mu, sigma, state_value = model(state)\n","    \n","    # create a normal distribution over the continuous action space\n","    m = Normal(loc=mu,scale=sigma)\n","    \n","    # and sample an action using the distribution\n","    action = m.sample()\n","    \n","    # save to action buffer\n","    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n","    \n","    # the action to take (left or right)\n","    return action.data.numpy()\n","\n","def finish_episode():\n","    \"\"\"\n","    Training code. Calculates actor and critic loss and performs backprop.\n","    \"\"\"\n","    R = 0\n","    saved_actions = model.saved_actions\n","    policy_losses = [] # list to save actor (policy) loss\n","    value_losses = [] # list to save critic (value) loss\n","    returns = [] # list to save the true values\n","    \n","    # calculate the true value using rewards returned from the environment\n","    for r in model.rewards[::-1]:\n","        # TODO compute the value at state x\n","        # via the reward and the discounted tail reward\n","        R = args.gamma*R + r\n","        \n","        returns.insert(0, R)\n","        \n","    # whiten the returns\n","    returns = torch.tensor(returns).float()\n","    returns = (returns - returns.mean()) / (returns.std() + eps)\n","    \n","    for (log_prob, value), R in zip(saved_actions, returns):\n","        # TODO compute the advantage via subtracting off value\n","        advantage = R-value.item()\n","        \n","        # TODO calculate actor (policy) loss, from log_prob (saved in select action)\n","        # and from advantage\n","        policy_loss = -log_prob*advantage\n","        # append this to policy_losses\n","        policy_losses.append(policy_loss)\n","        # TODO calculate critic (value) loss\n","        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n","    # reset gradients\n","    \n","    optimizer.zero_grad()\n","    \n","    # sum up all the values of policy_losses and value_losses\n","    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n","    \n","    # perform backprop\n","    loss.backward()\n","    optimizer.step()\n","    \n","    # reset rewards and action buffer\n","    del model.rewards[:]\n","    del model.saved_actions[:]\n","    \n","def main():\n","    running_reward = -100\n","    \n","    # run infinitely many episodes, until performance criteria met\n","    episodic_rewards = []\n","    episodes = []\n","    \n","    for i_episode in count(1):\n","        # reset environment and episode reward\n","        state = env.reset()\n","        ep_reward = 0\n","\n","        for t in range(1, 2500):\n","            # select action from policy\n","            action = select_action(state)\n","            \n","            # take the action\n","            state, reward, done, _ = env.step(action)\n","            \n","            if args.render and i_episode % 100 == 0:\n","                env.render()\n","    \n","            model.rewards.append(reward)\n","            ep_reward += reward\n","            if done:\n","                episodes.append(i_episode) # added\n","                episodic_rewards.append(ep_reward)\n","                break\n","                \n","        # update cumulative reward\n","        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n","        \n","        # perform backprop\n","        finish_episode()\n","        \n","        # log results\n","        if i_episode % args.log_interval == 0:\n","            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n","                  i_episode, ep_reward, running_reward))\n","            \n","        # check if we have \"solved\" the problem\n","        #if running_reward > 200:\n","        if i_episode > 6000:\n","            print(\"Solved! Running reward is now {} and \"\n","                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n","\n","            # TODO plot episodic_rewards --- submit this plot with your code\n","            plt.figure\n","            plt.plot(episodes, episodic_rewards)\n","            break\n","            \n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 10\tLast reward: -54.27\tAverage reward: -148.28\n","Episode 20\tLast reward: -248.84\tAverage reward: -159.68\n","Episode 30\tLast reward: -132.31\tAverage reward: -169.70\n","Episode 40\tLast reward: -306.82\tAverage reward: -189.38\n","Episode 50\tLast reward: -298.98\tAverage reward: -190.97\n","Episode 60\tLast reward: -52.21\tAverage reward: -175.61\n","Episode 70\tLast reward: -161.41\tAverage reward: -203.55\n","Episode 80\tLast reward: -139.27\tAverage reward: -210.64\n","Episode 90\tLast reward: -80.62\tAverage reward: -206.73\n","Episode 100\tLast reward: -152.32\tAverage reward: -209.79\n","Episode 110\tLast reward: -135.64\tAverage reward: -166.81\n","Episode 120\tLast reward: -270.60\tAverage reward: -181.45\n","Episode 130\tLast reward: -3.26\tAverage reward: -171.95\n","Episode 140\tLast reward: -250.65\tAverage reward: -206.45\n","Episode 150\tLast reward: -312.92\tAverage reward: -188.63\n","Episode 160\tLast reward: -183.09\tAverage reward: -188.15\n","Episode 170\tLast reward: -147.20\tAverage reward: -168.86\n","Episode 180\tLast reward: -201.31\tAverage reward: -186.82\n","Episode 190\tLast reward: -85.46\tAverage reward: -199.72\n","Episode 200\tLast reward: -74.06\tAverage reward: -178.49\n","Episode 210\tLast reward: -116.03\tAverage reward: -191.76\n","Episode 220\tLast reward: -241.66\tAverage reward: -184.46\n","Episode 230\tLast reward: -125.25\tAverage reward: -183.66\n","Episode 240\tLast reward: -73.64\tAverage reward: -176.71\n","Episode 250\tLast reward: -79.75\tAverage reward: -167.16\n","Episode 260\tLast reward: -132.42\tAverage reward: -161.99\n","Episode 270\tLast reward: -178.98\tAverage reward: -167.09\n","Episode 280\tLast reward: 92.97\tAverage reward: -166.04\n","Episode 290\tLast reward: -177.98\tAverage reward: -153.47\n","Episode 300\tLast reward: -13.15\tAverage reward: -169.23\n","Episode 310\tLast reward: -72.80\tAverage reward: -188.20\n","Episode 320\tLast reward: -90.01\tAverage reward: -170.08\n","Episode 330\tLast reward: -292.58\tAverage reward: -182.50\n","Episode 340\tLast reward: -448.63\tAverage reward: -187.72\n","Episode 350\tLast reward: -30.69\tAverage reward: -174.18\n","Episode 360\tLast reward: -228.33\tAverage reward: -193.38\n","Episode 370\tLast reward: -318.43\tAverage reward: -190.34\n","Episode 380\tLast reward: -115.19\tAverage reward: -160.19\n","Episode 390\tLast reward: -48.84\tAverage reward: -152.16\n","Episode 400\tLast reward: -293.31\tAverage reward: -171.42\n","Episode 410\tLast reward: -295.33\tAverage reward: -155.29\n","Episode 420\tLast reward: -304.49\tAverage reward: -191.77\n","Episode 430\tLast reward: -167.29\tAverage reward: -179.10\n","Episode 440\tLast reward: -291.57\tAverage reward: -186.40\n","Episode 450\tLast reward: -113.74\tAverage reward: -166.17\n","Episode 460\tLast reward: -106.87\tAverage reward: -164.27\n","Episode 470\tLast reward: 7.51\tAverage reward: -166.22\n","Episode 480\tLast reward: -295.66\tAverage reward: -177.40\n","Episode 490\tLast reward: -140.40\tAverage reward: -169.23\n","Episode 500\tLast reward: -150.36\tAverage reward: -156.33\n","Episode 510\tLast reward: -70.34\tAverage reward: -151.34\n","Episode 520\tLast reward: -87.41\tAverage reward: -147.58\n","Episode 530\tLast reward: -50.34\tAverage reward: -142.36\n","Episode 540\tLast reward: -97.95\tAverage reward: -145.01\n","Episode 550\tLast reward: -292.34\tAverage reward: -156.05\n","Episode 560\tLast reward: -185.40\tAverage reward: -162.13\n","Episode 570\tLast reward: -171.43\tAverage reward: -145.95\n","Episode 580\tLast reward: 7.46\tAverage reward: -139.64\n","Episode 590\tLast reward: -69.46\tAverage reward: -140.94\n","Episode 600\tLast reward: -64.45\tAverage reward: -168.53\n","Episode 610\tLast reward: -164.10\tAverage reward: -174.60\n","Episode 620\tLast reward: -290.87\tAverage reward: -170.01\n","Episode 630\tLast reward: -107.61\tAverage reward: -174.29\n","Episode 640\tLast reward: -136.24\tAverage reward: -173.50\n","Episode 650\tLast reward: -77.96\tAverage reward: -175.12\n","Episode 660\tLast reward: -121.80\tAverage reward: -163.53\n","Episode 670\tLast reward: -78.58\tAverage reward: -168.79\n","Episode 680\tLast reward: -356.80\tAverage reward: -171.22\n","Episode 690\tLast reward: -221.86\tAverage reward: -168.81\n","Episode 700\tLast reward: -168.69\tAverage reward: -164.61\n","Episode 710\tLast reward: -115.31\tAverage reward: -159.44\n","Episode 720\tLast reward: -284.70\tAverage reward: -149.44\n","Episode 730\tLast reward: -268.01\tAverage reward: -167.82\n","Episode 740\tLast reward: -157.53\tAverage reward: -171.99\n","Episode 750\tLast reward: -257.96\tAverage reward: -168.60\n","Episode 760\tLast reward: -96.16\tAverage reward: -146.41\n","Episode 770\tLast reward: -204.18\tAverage reward: -162.33\n","Episode 780\tLast reward: -107.23\tAverage reward: -145.88\n","Episode 790\tLast reward: -365.74\tAverage reward: -144.87\n","Episode 800\tLast reward: -180.74\tAverage reward: -155.94\n","Episode 810\tLast reward: -156.67\tAverage reward: -142.22\n","Episode 820\tLast reward: -109.97\tAverage reward: -154.46\n","Episode 830\tLast reward: -91.96\tAverage reward: -152.42\n","Episode 840\tLast reward: -251.09\tAverage reward: -153.47\n","Episode 850\tLast reward: -42.73\tAverage reward: -140.18\n","Episode 860\tLast reward: -78.92\tAverage reward: -130.24\n","Episode 870\tLast reward: -308.77\tAverage reward: -129.67\n","Episode 880\tLast reward: -61.10\tAverage reward: -146.28\n","Episode 890\tLast reward: -156.46\tAverage reward: -152.45\n","Episode 900\tLast reward: -234.48\tAverage reward: -166.84\n","Episode 910\tLast reward: -62.70\tAverage reward: -159.48\n","Episode 920\tLast reward: -162.05\tAverage reward: -154.28\n","Episode 930\tLast reward: -350.63\tAverage reward: -152.80\n","Episode 940\tLast reward: -63.25\tAverage reward: -138.08\n","Episode 950\tLast reward: -74.28\tAverage reward: -145.55\n","Episode 960\tLast reward: -122.42\tAverage reward: -137.41\n","Episode 970\tLast reward: -133.17\tAverage reward: -144.59\n","Episode 980\tLast reward: -56.50\tAverage reward: -146.64\n","Episode 990\tLast reward: -284.18\tAverage reward: -157.32\n","Episode 1000\tLast reward: -295.58\tAverage reward: -148.09\n","Episode 1010\tLast reward: -113.27\tAverage reward: -160.26\n","Episode 1020\tLast reward: -125.39\tAverage reward: -154.56\n","Episode 1030\tLast reward: -134.81\tAverage reward: -141.97\n","Episode 1040\tLast reward: -91.88\tAverage reward: -132.92\n","Episode 1050\tLast reward: -262.58\tAverage reward: -146.50\n","Episode 1060\tLast reward: -89.15\tAverage reward: -142.06\n","Episode 1070\tLast reward: -111.46\tAverage reward: -129.51\n","Episode 1080\tLast reward: -115.45\tAverage reward: -137.00\n","Episode 1090\tLast reward: -87.89\tAverage reward: -158.01\n","Episode 1100\tLast reward: -243.34\tAverage reward: -182.55\n","Episode 1110\tLast reward: -459.36\tAverage reward: -169.27\n","Episode 1120\tLast reward: -152.74\tAverage reward: -158.49\n","Episode 1130\tLast reward: -267.05\tAverage reward: -161.49\n","Episode 1140\tLast reward: -127.84\tAverage reward: -142.40\n","Episode 1150\tLast reward: -234.27\tAverage reward: -159.81\n","Episode 1160\tLast reward: -90.55\tAverage reward: -148.56\n","Episode 1170\tLast reward: -126.26\tAverage reward: -152.20\n","Episode 1180\tLast reward: -69.41\tAverage reward: -144.60\n","Episode 1190\tLast reward: -130.14\tAverage reward: -153.28\n","Episode 1200\tLast reward: -284.49\tAverage reward: -144.76\n","Episode 1210\tLast reward: -80.34\tAverage reward: -143.28\n","Episode 1220\tLast reward: -59.86\tAverage reward: -136.47\n","Episode 1230\tLast reward: -48.43\tAverage reward: -119.42\n","Episode 1240\tLast reward: -69.40\tAverage reward: -109.01\n","Episode 1250\tLast reward: -106.81\tAverage reward: -130.92\n","Episode 1260\tLast reward: -65.84\tAverage reward: -128.73\n","Episode 1270\tLast reward: -41.86\tAverage reward: -138.01\n","Episode 1280\tLast reward: -84.67\tAverage reward: -134.23\n","Episode 1290\tLast reward: -331.84\tAverage reward: -149.61\n","Episode 1300\tLast reward: -94.24\tAverage reward: -153.29\n","Episode 1310\tLast reward: -163.47\tAverage reward: -151.06\n","Episode 1320\tLast reward: -97.76\tAverage reward: -136.32\n","Episode 1330\tLast reward: -61.06\tAverage reward: -129.02\n","Episode 1340\tLast reward: -64.75\tAverage reward: -132.76\n","Episode 1350\tLast reward: -248.37\tAverage reward: -126.45\n","Episode 1360\tLast reward: -105.77\tAverage reward: -137.63\n","Episode 1370\tLast reward: -65.03\tAverage reward: -128.61\n","Episode 1380\tLast reward: -271.88\tAverage reward: -149.40\n","Episode 1390\tLast reward: -214.18\tAverage reward: -145.51\n","Episode 1400\tLast reward: -118.93\tAverage reward: -163.78\n","Episode 1410\tLast reward: -268.33\tAverage reward: -161.66\n","Episode 1420\tLast reward: -44.06\tAverage reward: -143.12\n","Episode 1430\tLast reward: -99.74\tAverage reward: -129.12\n","Episode 1440\tLast reward: -162.46\tAverage reward: -126.96\n","Episode 1450\tLast reward: -124.10\tAverage reward: -130.57\n","Episode 1460\tLast reward: -213.85\tAverage reward: -135.62\n","Episode 1470\tLast reward: -74.62\tAverage reward: -153.89\n","Episode 1480\tLast reward: -115.13\tAverage reward: -150.22\n","Episode 1490\tLast reward: -204.85\tAverage reward: -139.88\n","Episode 1500\tLast reward: -95.48\tAverage reward: -131.04\n","Episode 1510\tLast reward: -172.64\tAverage reward: -132.71\n","Episode 1520\tLast reward: -155.07\tAverage reward: -123.77\n","Episode 1530\tLast reward: -112.15\tAverage reward: -131.53\n","Episode 1540\tLast reward: -270.13\tAverage reward: -152.40\n","Episode 1550\tLast reward: -118.44\tAverage reward: -165.46\n","Episode 1560\tLast reward: -264.23\tAverage reward: -160.34\n","Episode 1570\tLast reward: -252.82\tAverage reward: -151.68\n","Episode 1580\tLast reward: -124.05\tAverage reward: -136.90\n","Episode 1590\tLast reward: -92.77\tAverage reward: -126.17\n","Episode 1600\tLast reward: -87.51\tAverage reward: -120.32\n","Episode 1610\tLast reward: -204.51\tAverage reward: -121.31\n","Episode 1620\tLast reward: -17.38\tAverage reward: -119.00\n","Episode 1630\tLast reward: -96.02\tAverage reward: -134.13\n","Episode 1640\tLast reward: -57.84\tAverage reward: -126.31\n","Episode 1650\tLast reward: -124.32\tAverage reward: -115.82\n","Episode 1660\tLast reward: -191.62\tAverage reward: -132.81\n","Episode 1670\tLast reward: -90.31\tAverage reward: -141.78\n","Episode 1680\tLast reward: -148.61\tAverage reward: -142.36\n","Episode 1690\tLast reward: -161.33\tAverage reward: -122.83\n","Episode 1700\tLast reward: -153.68\tAverage reward: -125.92\n","Episode 1710\tLast reward: -90.57\tAverage reward: -129.11\n","Episode 1720\tLast reward: -329.89\tAverage reward: -123.41\n","Episode 1730\tLast reward: -58.22\tAverage reward: -136.99\n","Episode 1740\tLast reward: -118.81\tAverage reward: -130.08\n","Episode 1750\tLast reward: -81.07\tAverage reward: -124.25\n","Episode 1760\tLast reward: -99.30\tAverage reward: -118.22\n","Episode 1770\tLast reward: -118.12\tAverage reward: -117.12\n","Episode 1780\tLast reward: -164.93\tAverage reward: -123.75\n","Episode 1790\tLast reward: -178.05\tAverage reward: -136.51\n","Episode 1800\tLast reward: -182.87\tAverage reward: -144.55\n","Episode 1810\tLast reward: -66.05\tAverage reward: -127.45\n","Episode 1820\tLast reward: -56.35\tAverage reward: -154.10\n","Episode 1830\tLast reward: -193.41\tAverage reward: -136.63\n","Episode 1840\tLast reward: -99.82\tAverage reward: -152.42\n","Episode 1850\tLast reward: -46.34\tAverage reward: -133.97\n","Episode 1860\tLast reward: -280.75\tAverage reward: -163.60\n","Episode 1870\tLast reward: -124.30\tAverage reward: -150.63\n","Episode 1880\tLast reward: -95.11\tAverage reward: -141.02\n","Episode 1890\tLast reward: -214.95\tAverage reward: -143.02\n","Episode 1900\tLast reward: -261.93\tAverage reward: -158.28\n","Episode 1910\tLast reward: -236.48\tAverage reward: -157.34\n","Episode 1920\tLast reward: -52.89\tAverage reward: -138.23\n","Episode 1930\tLast reward: -137.13\tAverage reward: -155.36\n","Episode 1940\tLast reward: -47.77\tAverage reward: -134.14\n","Episode 1950\tLast reward: -57.25\tAverage reward: -130.58\n","Episode 1960\tLast reward: -179.66\tAverage reward: -142.23\n","Episode 1970\tLast reward: -87.92\tAverage reward: -145.75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wzl0MsvEUxCi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}