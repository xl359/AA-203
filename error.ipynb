{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "error.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6pUWhT_MfC_",
        "colab_type": "code",
        "outputId": "5420d965-4136-4a27-83b6-3336f6e92fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Jun  5 17:08:54 2020\n",
        "\n",
        "@author: Admin\n",
        "\"\"\"\n",
        "\n",
        "# Quadcopter Env\n",
        "\n",
        "import easydict\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "import logging\n",
        "import logging.handlers\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from quad_env import QuadEnv\n",
        "\n",
        "# Cart Pole\n",
        "# based on:\n",
        "# https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "    \"gamma\": 0.99,\n",
        "    \"seed\": 203,\n",
        "    \"render\": False,\n",
        "    \"log_interval\": 10,\n",
        "    \"write_logger\":True\n",
        "})\n",
        "\n",
        "# env = gym.make('LunarLanderContinuous-v2')\n",
        "env = QuadEnv()\n",
        "\n",
        "# env.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "# state_dim = env.observation_space.shape[0]\n",
        "state_dim = 18\n",
        "# action_dim = env.action_space.shape[0]\n",
        "action_dim = 4\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    \"\"\"\n",
        "    implements both actor and critic in one model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim1=64, hidden_dim2=64, output_dim=128):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(state_dim, hidden_dim1)\n",
        "        self.affine2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.affine3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.act1 = nn.Tanh()\n",
        "        # actor's layer\n",
        "        self.action_mean = nn.Linear(output_dim, action_dim)\n",
        "        self.action_var = nn.Linear(output_dim, action_dim)\n",
        "        # critic's layer\n",
        "        self.value_head = nn.Linear(output_dim, 1)\n",
        "        # action & reward buffer\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward of both actor and critic\n",
        "        \"\"\"\n",
        "        # TODO map input to:\n",
        "        # mean of action distribution,\n",
        "        # variance of action distribution (pass this through a non-negative function),\n",
        "        # state value\n",
        "\n",
        "        input_x = x\n",
        "        x = self.act1(self.affine1(x))\n",
        "        x = self.act1(self.affine2(x))\n",
        "        x = self.act1(self.affine3(x))\n",
        "        action_mean = self.action_mean(x)\n",
        "        action_var = F.softplus(self.action_var(x))\n",
        "        action_var = torch.add(action_var, 1e-10)\n",
        "        state_values = self.value_head(x)  # <= Value Function not value of state\n",
        "        if any(torch.isnan(x)) or any(torch.isnan(action_mean)) or any(torch.isnan(action_var)):\n",
        "            print('NaN in forward pass')\n",
        "\n",
        "        return 1000.0 * action_mean, 1000.0 * action_var, state_values\n",
        "\n",
        "\n",
        "model = Policy().float()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    mu, sigma, state_value = model(state)\n",
        "    # sigma += 10**-8\n",
        "\n",
        "    # create a normal distribution over the continuous action space\n",
        "    m = Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    # and sample an action using the distribution\n",
        "    action = m.sample()\n",
        "\n",
        "    # save to action buffer\n",
        "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "\n",
        "    # the action to take (left or right)\n",
        "    return action.data.numpy()\n",
        "\n",
        "\n",
        "def finish_episode():\n",
        "    \"\"\"\n",
        "    Training code. Calculates actor and critic loss and performs backprop.\n",
        "    \"\"\"\n",
        "    R = 0\n",
        "    saved_actions = model.saved_actions\n",
        "    policy_losses = []  # list to save actor (policy) loss\n",
        "    value_losses = []  # list to save critic (value) loss\n",
        "    returns = []  # list to save the true values\n",
        "\n",
        "    # calculate the true value using rewards returned from the environment\n",
        "    #for r in model.rewards[::-1]:\n",
        "    for i in range(len(model.rewards)):\n",
        "        # TODO compute the value at state x\n",
        "        # via the reward and the discounted tail reward\n",
        "        r = model.rewards[i]\n",
        "        if i < len(model.rewards) - 1:\n",
        "          value_next = saved_actions[i + 1][1].item()\n",
        "        else:\n",
        "          value_next = 0\n",
        "        R = args.gamma * value_next + r\n",
        "\n",
        "        returns.insert(0, R)\n",
        "\n",
        "    # whiten the returns\n",
        "    returns = torch.tensor(returns).float()\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    for (log_prob, value), R in zip(saved_actions, returns):\n",
        "        # TODO compute the advantage via subtracting off value\n",
        "        advantage = R - value.item()\n",
        "\n",
        "        # TODO calculate actor (policy) loss, from log_prob (saved in select action)\n",
        "        # and from advantage\n",
        "        policy_loss = -log_prob * advantage\n",
        "        # append this to policy_losses\n",
        "        policy_losses.append(policy_loss)\n",
        "        # TODO calculate critic (value) loss\n",
        "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
        "    # reset gradients\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # sum up all the values of policy_losses and value_losses\n",
        "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "\n",
        "    # perform backprop\n",
        "    loss.backward()\n",
        "    # gradient clipping to solve exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    # reset rewards and action buffer\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]\n",
        "\n",
        "\n",
        "def main():\n",
        "    if args.write_logger:\n",
        "        log_filename = 'training_log.txt'\n",
        "        train_logger = logging.getLogger('TrainLogger')\n",
        "        train_logger.setLevel(logging.DEBUG)\n",
        "        handler = logging.handlers.RotatingFileHandler(log_filename, maxBytes=10*1024*1024, backupCount=5)\n",
        "        train_logger.addHandler(handler)\n",
        "\n",
        "    running_reward = -8000\n",
        "\n",
        "    # run infinitely many episodes, until performance criteria met\n",
        "    episodic_rewards = []\n",
        "    episodes = []\n",
        "\n",
        "    for i_episode in count(1):\n",
        "        # reset environment and episode reward\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "\n",
        "        for t in range(1, 300):\n",
        "            # select action from policy\n",
        "            action = select_action(state)\n",
        "            if any(np.isnan(action)):\n",
        "                print('action is NaN')\n",
        "\n",
        "            # take the action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if args.render and i_episode % 100 == 0:\n",
        "                env.render()\n",
        "\n",
        "            if args.write_logger:\n",
        "                train_logger.debug('episode {0}, step {1}, state {2}, action {3}, reward {4}'.format(i_episode, t, state, action, reward))\n",
        "\n",
        "            model.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                episodes.append(i_episode)  # added\n",
        "                episodic_rewards.append(ep_reward)\n",
        "                break\n",
        "\n",
        "        # update cumulative reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # perform backprop\n",
        "        finish_episode()\n",
        "\n",
        "        # log results\n",
        "        if i_episode % args.log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                i_episode, ep_reward, running_reward))\n",
        "\n",
        "        # check if we have \"solved\" the problem\n",
        "        # if running_reward > 200:\n",
        "        if i_episode > 6000:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "\n",
        "            # TODO plot episodic_rewards --- submit this plot with your code\n",
        "            plt.figure()\n",
        "            plt.plot(episodes, episodic_rewards)\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 10\tLast reward: -1461.76\tAverage reward: -5327.44\n",
            "Episode 20\tLast reward: -978.14\tAverage reward: -3770.94\n",
            "Episode 30\tLast reward: -1087.20\tAverage reward: -2906.01\n",
            "Episode 40\tLast reward: -1930.15\tAverage reward: -2264.26\n",
            "Episode 50\tLast reward: -1043.54\tAverage reward: -1821.27\n",
            "Episode 60\tLast reward: -1691.61\tAverage reward: -1539.02\n",
            "Episode 70\tLast reward: -1394.95\tAverage reward: -1474.12\n",
            "Episode 80\tLast reward: -1100.29\tAverage reward: -1457.52\n",
            "Episode 90\tLast reward: -1650.87\tAverage reward: -1408.94\n",
            "Episode 100\tLast reward: -976.39\tAverage reward: -1289.78\n",
            "Episode 110\tLast reward: -1298.82\tAverage reward: -1193.99\n",
            "Episode 120\tLast reward: -882.04\tAverage reward: -1100.40\n",
            "Episode 130\tLast reward: -1284.28\tAverage reward: -1118.33\n",
            "Episode 140\tLast reward: -1225.89\tAverage reward: -1129.32\n",
            "Episode 150\tLast reward: -963.51\tAverage reward: -1179.05\n",
            "Episode 160\tLast reward: -2578.83\tAverage reward: -1428.39\n",
            "Episode 170\tLast reward: -2576.47\tAverage reward: -2111.12\n",
            "Episode 180\tLast reward: -4018.53\tAverage reward: -2678.82\n",
            "Episode 190\tLast reward: -1677.33\tAverage reward: -3240.30\n",
            "Episode 200\tLast reward: -6325.72\tAverage reward: -3714.03\n",
            "Episode 210\tLast reward: -2829.03\tAverage reward: -3380.77\n",
            "Episode 220\tLast reward: -839.90\tAverage reward: -2570.92\n",
            "Episode 230\tLast reward: -1169.69\tAverage reward: -2235.15\n",
            "Episode 240\tLast reward: -4391.59\tAverage reward: -2299.69\n",
            "Episode 250\tLast reward: -6052.66\tAverage reward: -2807.10\n",
            "Episode 260\tLast reward: -2095.76\tAverage reward: -3624.64\n",
            "Episode 270\tLast reward: -2269.12\tAverage reward: -4180.24\n",
            "Episode 280\tLast reward: -5006.88\tAverage reward: -4308.53\n",
            "Episode 290\tLast reward: -5439.51\tAverage reward: -4066.21\n",
            "Episode 300\tLast reward: -2417.87\tAverage reward: -4023.87\n",
            "Episode 310\tLast reward: -2356.65\tAverage reward: -4268.96\n",
            "Episode 320\tLast reward: -4620.88\tAverage reward: -4077.01\n",
            "Episode 330\tLast reward: -1596.30\tAverage reward: -3505.44\n",
            "Episode 340\tLast reward: -2845.53\tAverage reward: -3208.45\n",
            "Episode 350\tLast reward: -4610.46\tAverage reward: -3314.00\n",
            "Episode 360\tLast reward: -2076.76\tAverage reward: -3079.90\n",
            "Episode 370\tLast reward: -5249.17\tAverage reward: -2857.15\n",
            "Episode 380\tLast reward: -1817.49\tAverage reward: -2798.94\n",
            "Episode 390\tLast reward: -4157.99\tAverage reward: -2973.82\n",
            "Episode 400\tLast reward: -3021.23\tAverage reward: -3578.33\n",
            "Episode 410\tLast reward: -3169.43\tAverage reward: -4066.23\n",
            "Episode 420\tLast reward: -2430.53\tAverage reward: -4253.68\n",
            "Episode 430\tLast reward: -4890.60\tAverage reward: -5103.47\n",
            "Episode 440\tLast reward: -1864.16\tAverage reward: -5047.20\n",
            "Episode 450\tLast reward: -1635.96\tAverage reward: -4329.55\n",
            "Episode 460\tLast reward: -4038.73\tAverage reward: -4390.61\n",
            "Episode 470\tLast reward: -2197.98\tAverage reward: -4312.44\n",
            "Episode 480\tLast reward: -1566.52\tAverage reward: -3553.44\n",
            "Episode 490\tLast reward: -4096.56\tAverage reward: -3212.14\n",
            "Episode 500\tLast reward: -2321.68\tAverage reward: -2978.76\n",
            "Episode 510\tLast reward: -3072.08\tAverage reward: -3330.93\n",
            "Episode 520\tLast reward: -4998.25\tAverage reward: -3943.26\n",
            "Episode 530\tLast reward: -3778.49\tAverage reward: -4279.40\n",
            "Episode 540\tLast reward: -2610.86\tAverage reward: -3817.25\n",
            "Episode 550\tLast reward: -1197.39\tAverage reward: -3180.93\n",
            "Episode 560\tLast reward: -1834.62\tAverage reward: -2685.11\n",
            "Episode 570\tLast reward: -1320.38\tAverage reward: -2565.01\n",
            "Episode 580\tLast reward: -2598.72\tAverage reward: -2365.78\n",
            "Episode 590\tLast reward: -2388.72\tAverage reward: -2248.35\n",
            "Episode 600\tLast reward: -2659.44\tAverage reward: -2199.22\n",
            "Episode 610\tLast reward: -3080.83\tAverage reward: -2107.59\n",
            "Episode 620\tLast reward: -1698.04\tAverage reward: -1935.54\n",
            "Episode 630\tLast reward: -2376.44\tAverage reward: -1980.20\n",
            "Episode 640\tLast reward: -2774.93\tAverage reward: -2253.21\n",
            "Episode 650\tLast reward: -3698.80\tAverage reward: -3037.05\n",
            "Episode 660\tLast reward: -5382.98\tAverage reward: -4563.65\n",
            "Episode 670\tLast reward: -19622.19\tAverage reward: -6834.97\n",
            "Episode 680\tLast reward: -4152.71\tAverage reward: -8258.23\n",
            "Episode 690\tLast reward: -10048.26\tAverage reward: -7965.20\n",
            "Episode 700\tLast reward: -4032.32\tAverage reward: -7233.30\n",
            "Episode 710\tLast reward: -4800.36\tAverage reward: -7529.83\n",
            "Episode 720\tLast reward: -6776.42\tAverage reward: -8701.66\n",
            "Episode 730\tLast reward: -4853.77\tAverage reward: -10062.82\n",
            "Episode 740\tLast reward: -11112.41\tAverage reward: -10954.05\n",
            "Episode 750\tLast reward: -17249.83\tAverage reward: -12013.16\n",
            "Episode 760\tLast reward: -20049.97\tAverage reward: -15226.80\n",
            "Episode 770\tLast reward: -27900.94\tAverage reward: -14709.72\n",
            "Episode 780\tLast reward: -7862.34\tAverage reward: -12163.46\n",
            "Episode 790\tLast reward: -9583.25\tAverage reward: -11256.95\n",
            "Episode 800\tLast reward: -8263.94\tAverage reward: -10937.50\n",
            "Episode 810\tLast reward: -8317.81\tAverage reward: -11138.41\n",
            "Episode 820\tLast reward: -6420.81\tAverage reward: -12069.09\n",
            "Episode 830\tLast reward: -6564.62\tAverage reward: -11409.54\n",
            "Episode 840\tLast reward: -6510.35\tAverage reward: -13519.50\n",
            "Episode 850\tLast reward: -4418.39\tAverage reward: -14711.26\n",
            "Episode 860\tLast reward: -4630.91\tAverage reward: -11166.12\n",
            "Episode 870\tLast reward: -5675.13\tAverage reward: -8514.70\n",
            "Episode 880\tLast reward: -3794.75\tAverage reward: -6695.06\n",
            "Episode 890\tLast reward: -6485.68\tAverage reward: -5587.71\n",
            "Episode 900\tLast reward: -3524.43\tAverage reward: -5181.72\n",
            "Episode 910\tLast reward: -4072.73\tAverage reward: -4964.23\n",
            "Episode 920\tLast reward: -6068.85\tAverage reward: -5654.43\n",
            "Episode 930\tLast reward: -5053.83\tAverage reward: -5567.93\n",
            "Episode 940\tLast reward: -4892.75\tAverage reward: -5134.87\n",
            "Episode 950\tLast reward: -3230.48\tAverage reward: -4475.52\n",
            "Episode 960\tLast reward: -4149.89\tAverage reward: -3769.93\n",
            "Episode 970\tLast reward: -4200.22\tAverage reward: -3803.22\n",
            "Episode 980\tLast reward: -3096.08\tAverage reward: -3750.98\n",
            "Episode 990\tLast reward: -2681.24\tAverage reward: -3558.92\n",
            "Episode 1000\tLast reward: -2560.26\tAverage reward: -2834.98\n",
            "Episode 1010\tLast reward: -1475.37\tAverage reward: -2511.86\n",
            "Episode 1020\tLast reward: -4508.42\tAverage reward: -2946.78\n",
            "Episode 1030\tLast reward: -1793.84\tAverage reward: -3364.69\n",
            "Episode 1040\tLast reward: -3775.59\tAverage reward: -3380.33\n",
            "Episode 1050\tLast reward: -1858.30\tAverage reward: -3686.38\n",
            "Episode 1060\tLast reward: -8007.94\tAverage reward: -5475.82\n",
            "Episode 1070\tLast reward: -5942.22\tAverage reward: -5993.72\n",
            "Episode 1080\tLast reward: -3577.23\tAverage reward: -6036.94\n",
            "Episode 1090\tLast reward: -6788.74\tAverage reward: -6281.06\n",
            "Episode 1100\tLast reward: -2184.42\tAverage reward: -5673.18\n",
            "Episode 1110\tLast reward: -4949.19\tAverage reward: -5852.62\n",
            "Episode 1120\tLast reward: -7089.09\tAverage reward: -5604.71\n",
            "Episode 1130\tLast reward: -4438.18\tAverage reward: -5767.63\n",
            "Episode 1140\tLast reward: -8727.61\tAverage reward: -6239.50\n",
            "Episode 1150\tLast reward: -7442.46\tAverage reward: -5970.14\n",
            "Episode 1160\tLast reward: -3912.06\tAverage reward: -5838.48\n",
            "Episode 1170\tLast reward: -7649.51\tAverage reward: -6272.47\n",
            "Episode 1180\tLast reward: -3812.35\tAverage reward: -6781.72\n",
            "Episode 1190\tLast reward: -12849.40\tAverage reward: -6564.93\n",
            "Episode 1200\tLast reward: -10857.81\tAverage reward: -7098.13\n",
            "Episode 1210\tLast reward: -6106.84\tAverage reward: -7369.65\n",
            "Episode 1220\tLast reward: -10628.04\tAverage reward: -7611.61\n",
            "Episode 1230\tLast reward: -5032.20\tAverage reward: -7110.24\n",
            "Episode 1240\tLast reward: -7902.41\tAverage reward: -6994.96\n",
            "Episode 1250\tLast reward: -3340.83\tAverage reward: -6695.41\n",
            "Episode 1260\tLast reward: -6434.27\tAverage reward: -6341.87\n",
            "Episode 1270\tLast reward: -3016.07\tAverage reward: -5484.74\n",
            "Episode 1280\tLast reward: -6819.47\tAverage reward: -5289.28\n",
            "Episode 1290\tLast reward: -4902.36\tAverage reward: -5507.33\n",
            "Episode 1300\tLast reward: -8123.95\tAverage reward: -6187.04\n",
            "Episode 1310\tLast reward: -4679.50\tAverage reward: -6116.14\n",
            "Episode 1320\tLast reward: -8434.24\tAverage reward: -5801.64\n",
            "Episode 1330\tLast reward: -7257.40\tAverage reward: -6044.90\n",
            "Episode 1340\tLast reward: -7877.06\tAverage reward: -6083.12\n",
            "Episode 1350\tLast reward: -8122.86\tAverage reward: -6460.87\n",
            "Episode 1360\tLast reward: -5843.86\tAverage reward: -6301.56\n",
            "Episode 1370\tLast reward: -3812.29\tAverage reward: -6234.22\n",
            "Episode 1380\tLast reward: -9124.58\tAverage reward: -6713.70\n",
            "Episode 1390\tLast reward: -9283.50\tAverage reward: -6749.65\n",
            "Episode 1400\tLast reward: -10581.52\tAverage reward: -7124.21\n",
            "Episode 1410\tLast reward: -7683.43\tAverage reward: -6664.86\n",
            "Episode 1420\tLast reward: -5480.22\tAverage reward: -6764.62\n",
            "Episode 1430\tLast reward: -9290.27\tAverage reward: -6999.46\n",
            "Episode 1440\tLast reward: -5073.38\tAverage reward: -6763.68\n",
            "Episode 1450\tLast reward: -4444.97\tAverage reward: -6531.70\n",
            "Episode 1460\tLast reward: -4280.08\tAverage reward: -6515.29\n",
            "Episode 1470\tLast reward: -3743.85\tAverage reward: -6048.01\n",
            "Episode 1480\tLast reward: -3829.76\tAverage reward: -5968.78\n",
            "Episode 1490\tLast reward: -9834.81\tAverage reward: -6868.78\n",
            "Episode 1500\tLast reward: -7172.36\tAverage reward: -6832.97\n",
            "Episode 1510\tLast reward: -7153.58\tAverage reward: -7381.38\n",
            "Episode 1520\tLast reward: -5824.84\tAverage reward: -7625.12\n",
            "Episode 1530\tLast reward: -8230.77\tAverage reward: -7340.27\n",
            "Episode 1540\tLast reward: -3546.62\tAverage reward: -6824.61\n",
            "Episode 1550\tLast reward: -6926.25\tAverage reward: -7264.98\n",
            "Episode 1560\tLast reward: -5340.42\tAverage reward: -7162.66\n",
            "Episode 1570\tLast reward: -2320.05\tAverage reward: -7253.19\n",
            "Episode 1580\tLast reward: -6214.40\tAverage reward: -7044.03\n",
            "Episode 1590\tLast reward: -3347.79\tAverage reward: -6680.30\n",
            "Episode 1600\tLast reward: -3906.85\tAverage reward: -6365.33\n",
            "Episode 1610\tLast reward: -6494.47\tAverage reward: -6363.91\n",
            "Episode 1620\tLast reward: -8356.92\tAverage reward: -5737.76\n",
            "Episode 1630\tLast reward: -3386.89\tAverage reward: -6239.38\n",
            "Episode 1640\tLast reward: -6525.66\tAverage reward: -6540.55\n",
            "Episode 1650\tLast reward: -8594.11\tAverage reward: -6454.21\n",
            "Episode 1660\tLast reward: -6125.69\tAverage reward: -6377.70\n",
            "Episode 1670\tLast reward: -4040.77\tAverage reward: -6161.16\n",
            "Episode 1680\tLast reward: -7473.82\tAverage reward: -5749.99\n",
            "Episode 1690\tLast reward: -6395.73\tAverage reward: -5909.67\n",
            "Episode 1700\tLast reward: -3405.93\tAverage reward: -5501.84\n",
            "Episode 1710\tLast reward: -4974.05\tAverage reward: -5489.86\n",
            "Episode 1720\tLast reward: -8241.58\tAverage reward: -5361.92\n",
            "Episode 1730\tLast reward: -5058.88\tAverage reward: -5418.20\n",
            "Episode 1740\tLast reward: -7943.17\tAverage reward: -5704.05\n",
            "Episode 1750\tLast reward: -3591.51\tAverage reward: -5271.15\n",
            "Episode 1760\tLast reward: -4038.81\tAverage reward: -5478.46\n",
            "Episode 1770\tLast reward: -4111.70\tAverage reward: -5680.55\n",
            "Episode 1780\tLast reward: -3000.25\tAverage reward: -5909.16\n",
            "Episode 1790\tLast reward: -4329.86\tAverage reward: -6138.13\n",
            "Episode 1800\tLast reward: -10177.58\tAverage reward: -6329.43\n",
            "Episode 1810\tLast reward: -8893.08\tAverage reward: -6140.10\n",
            "Episode 1820\tLast reward: -8820.30\tAverage reward: -5836.49\n",
            "Episode 1830\tLast reward: -7180.36\tAverage reward: -5969.10\n",
            "Episode 1840\tLast reward: -5360.89\tAverage reward: -6402.23\n",
            "Episode 1850\tLast reward: -6923.67\tAverage reward: -6296.07\n",
            "Episode 1860\tLast reward: -7760.82\tAverage reward: -6400.01\n",
            "Episode 1870\tLast reward: -9224.53\tAverage reward: -6244.86\n",
            "Episode 1880\tLast reward: -7497.81\tAverage reward: -6400.31\n",
            "Episode 1890\tLast reward: -6316.31\tAverage reward: -6078.95\n",
            "Episode 1900\tLast reward: -4670.29\tAverage reward: -5761.62\n",
            "Episode 1910\tLast reward: -2478.17\tAverage reward: -5355.97\n",
            "Episode 1920\tLast reward: -4118.65\tAverage reward: -5293.58\n",
            "Episode 1930\tLast reward: -4639.29\tAverage reward: -5196.34\n",
            "Episode 1940\tLast reward: -10210.61\tAverage reward: -5381.91\n",
            "Episode 1950\tLast reward: -1822.22\tAverage reward: -5872.56\n",
            "Episode 1960\tLast reward: -4612.73\tAverage reward: -6127.41\n",
            "Episode 1970\tLast reward: -8032.35\tAverage reward: -6258.22\n",
            "Episode 1980\tLast reward: -6894.31\tAverage reward: -6215.99\n",
            "Episode 1990\tLast reward: -7717.05\tAverage reward: -5877.01\n",
            "Episode 2000\tLast reward: -10303.40\tAverage reward: -5938.92\n",
            "Episode 2010\tLast reward: -6207.81\tAverage reward: -6468.91\n",
            "Episode 2020\tLast reward: -3234.68\tAverage reward: -6070.20\n",
            "Episode 2030\tLast reward: -6123.83\tAverage reward: -6119.38\n",
            "Episode 2040\tLast reward: -6757.82\tAverage reward: -6167.11\n",
            "Episode 2050\tLast reward: -7274.63\tAverage reward: -6301.64\n",
            "Episode 2060\tLast reward: -11734.77\tAverage reward: -6710.94\n",
            "Episode 2070\tLast reward: -3886.50\tAverage reward: -6198.33\n",
            "Episode 2080\tLast reward: -5030.41\tAverage reward: -6334.02\n",
            "Episode 2090\tLast reward: -3013.38\tAverage reward: -6408.23\n",
            "Episode 2100\tLast reward: -4636.03\tAverage reward: -6594.34\n",
            "Episode 2110\tLast reward: -4761.44\tAverage reward: -6296.49\n",
            "Episode 2120\tLast reward: -2219.04\tAverage reward: -5782.65\n",
            "Episode 2130\tLast reward: -11184.60\tAverage reward: -6692.38\n",
            "Episode 2140\tLast reward: -9104.25\tAverage reward: -6633.17\n",
            "Episode 2150\tLast reward: -4511.51\tAverage reward: -6103.83\n",
            "Episode 2160\tLast reward: -6349.20\tAverage reward: -6004.94\n",
            "Episode 2170\tLast reward: -6935.07\tAverage reward: -6232.83\n",
            "Episode 2180\tLast reward: -3511.65\tAverage reward: -6865.04\n",
            "Episode 2190\tLast reward: -8870.77\tAverage reward: -6915.44\n",
            "Episode 2200\tLast reward: -6481.56\tAverage reward: -7075.83\n",
            "Episode 2210\tLast reward: -8147.33\tAverage reward: -7303.41\n",
            "Episode 2220\tLast reward: -6483.46\tAverage reward: -7555.23\n",
            "Episode 2230\tLast reward: -8413.79\tAverage reward: -7205.38\n",
            "Episode 2240\tLast reward: -12300.95\tAverage reward: -7776.56\n",
            "Episode 2250\tLast reward: -5802.56\tAverage reward: -7587.18\n",
            "Episode 2260\tLast reward: -6762.66\tAverage reward: -7470.55\n",
            "Episode 2270\tLast reward: -8608.14\tAverage reward: -7290.87\n",
            "Episode 2280\tLast reward: -9323.56\tAverage reward: -7107.09\n",
            "Episode 2290\tLast reward: -7241.84\tAverage reward: -7525.79\n",
            "Episode 2300\tLast reward: -4557.60\tAverage reward: -7348.34\n",
            "Episode 2310\tLast reward: -9897.87\tAverage reward: -7681.14\n",
            "Episode 2320\tLast reward: -7834.76\tAverage reward: -7683.65\n",
            "Episode 2330\tLast reward: -10335.11\tAverage reward: -7759.38\n",
            "Episode 2340\tLast reward: -10502.50\tAverage reward: -8255.81\n",
            "Episode 2350\tLast reward: -3959.92\tAverage reward: -8505.51\n",
            "Episode 2360\tLast reward: -7545.73\tAverage reward: -8318.83\n",
            "Episode 2370\tLast reward: -8613.68\tAverage reward: -7859.73\n",
            "Episode 2380\tLast reward: -2288.87\tAverage reward: -7230.88\n",
            "Episode 2390\tLast reward: -3843.18\tAverage reward: -6779.91\n",
            "Episode 2400\tLast reward: -9306.80\tAverage reward: -7332.59\n",
            "Episode 2410\tLast reward: -8553.07\tAverage reward: -7725.56\n",
            "Episode 2420\tLast reward: -6679.22\tAverage reward: -7126.60\n",
            "Episode 2430\tLast reward: -3173.85\tAverage reward: -7221.25\n",
            "Episode 2440\tLast reward: -5934.93\tAverage reward: -6963.78\n",
            "Episode 2450\tLast reward: -9033.06\tAverage reward: -7499.34\n",
            "Episode 2460\tLast reward: -3501.95\tAverage reward: -7117.00\n",
            "Episode 2470\tLast reward: -9538.02\tAverage reward: -7479.74\n",
            "Episode 2480\tLast reward: -5851.50\tAverage reward: -7251.12\n",
            "Episode 2490\tLast reward: -11603.36\tAverage reward: -8545.09\n",
            "Episode 2500\tLast reward: -9919.27\tAverage reward: -8399.03\n",
            "Episode 2510\tLast reward: -10251.68\tAverage reward: -8745.30\n",
            "Episode 2520\tLast reward: -7339.90\tAverage reward: -9073.64\n",
            "Episode 2530\tLast reward: -6771.46\tAverage reward: -9022.78\n",
            "Episode 2540\tLast reward: -9935.10\tAverage reward: -9054.01\n",
            "Episode 2550\tLast reward: -9685.41\tAverage reward: -8461.95\n",
            "Episode 2560\tLast reward: -10808.39\tAverage reward: -8215.72\n",
            "Episode 2570\tLast reward: -10526.42\tAverage reward: -8218.20\n",
            "Episode 2580\tLast reward: -4977.27\tAverage reward: -7981.01\n",
            "Episode 2590\tLast reward: -9726.76\tAverage reward: -7849.94\n",
            "Episode 2600\tLast reward: -10079.53\tAverage reward: -7835.80\n",
            "Episode 2610\tLast reward: -8306.05\tAverage reward: -8241.48\n",
            "Episode 2620\tLast reward: -7972.83\tAverage reward: -8587.56\n",
            "Episode 2630\tLast reward: -4798.23\tAverage reward: -7442.14\n",
            "Episode 2640\tLast reward: -9224.29\tAverage reward: -8021.10\n",
            "Episode 2650\tLast reward: -4707.96\tAverage reward: -7809.93\n",
            "Episode 2660\tLast reward: -8768.06\tAverage reward: -8358.02\n",
            "Episode 2670\tLast reward: -7008.86\tAverage reward: -8627.17\n",
            "Episode 2680\tLast reward: -4479.11\tAverage reward: -8204.74\n",
            "Episode 2690\tLast reward: -4931.31\tAverage reward: -7882.62\n",
            "Episode 2700\tLast reward: -9859.88\tAverage reward: -8866.92\n",
            "Episode 2710\tLast reward: -4462.15\tAverage reward: -8003.86\n",
            "Episode 2720\tLast reward: -12535.35\tAverage reward: -8451.22\n",
            "Episode 2730\tLast reward: -7568.15\tAverage reward: -8253.35\n",
            "Episode 2740\tLast reward: -9717.02\tAverage reward: -8194.33\n",
            "Episode 2750\tLast reward: -13092.12\tAverage reward: -8480.29\n",
            "Episode 2760\tLast reward: -8570.19\tAverage reward: -8919.14\n",
            "Episode 2770\tLast reward: -7990.81\tAverage reward: -8421.09\n",
            "Episode 2780\tLast reward: -7618.56\tAverage reward: -8669.41\n",
            "Episode 2790\tLast reward: -11952.56\tAverage reward: -9357.87\n",
            "Episode 2800\tLast reward: -7846.21\tAverage reward: -9474.19\n",
            "Episode 2810\tLast reward: -16090.93\tAverage reward: -9842.41\n",
            "Episode 2820\tLast reward: -5877.29\tAverage reward: -10141.91\n",
            "Episode 2830\tLast reward: -9734.10\tAverage reward: -10117.61\n",
            "Episode 2840\tLast reward: -9094.94\tAverage reward: -9906.06\n",
            "Episode 2850\tLast reward: -10618.46\tAverage reward: -9499.02\n",
            "Episode 2860\tLast reward: -12750.83\tAverage reward: -9782.08\n",
            "Episode 2870\tLast reward: -5891.85\tAverage reward: -9154.96\n",
            "Episode 2880\tLast reward: -10115.84\tAverage reward: -8992.86\n",
            "Episode 2890\tLast reward: -10692.19\tAverage reward: -8625.71\n",
            "Episode 2900\tLast reward: -7364.74\tAverage reward: -8199.03\n",
            "Episode 2910\tLast reward: -9535.13\tAverage reward: -8643.60\n",
            "Episode 2920\tLast reward: -13913.74\tAverage reward: -8963.73\n",
            "Episode 2930\tLast reward: -10095.76\tAverage reward: -8903.26\n",
            "Episode 2940\tLast reward: -9671.74\tAverage reward: -8903.15\n",
            "Episode 2950\tLast reward: -6730.69\tAverage reward: -9027.09\n",
            "Episode 2960\tLast reward: -13393.99\tAverage reward: -9619.43\n",
            "Episode 2970\tLast reward: -8774.88\tAverage reward: -9586.85\n",
            "Episode 2980\tLast reward: -8858.33\tAverage reward: -9049.62\n",
            "Episode 2990\tLast reward: -11719.70\tAverage reward: -8779.69\n",
            "Episode 3000\tLast reward: -5115.28\tAverage reward: -8689.53\n",
            "Episode 3010\tLast reward: -10439.90\tAverage reward: -8568.05\n",
            "Episode 3020\tLast reward: -5326.34\tAverage reward: -8320.05\n",
            "Episode 3030\tLast reward: -9635.33\tAverage reward: -8120.50\n",
            "Episode 3040\tLast reward: -3990.53\tAverage reward: -8036.17\n",
            "Episode 3050\tLast reward: -5078.94\tAverage reward: -8106.76\n",
            "Episode 3060\tLast reward: -4210.92\tAverage reward: -7590.81\n",
            "Episode 3070\tLast reward: -5678.91\tAverage reward: -7347.96\n",
            "Episode 3080\tLast reward: -5363.29\tAverage reward: -7838.39\n",
            "Episode 3090\tLast reward: -3662.66\tAverage reward: -7435.59\n",
            "Episode 3100\tLast reward: -10621.05\tAverage reward: -7576.32\n",
            "Episode 3110\tLast reward: -8446.19\tAverage reward: -7739.25\n",
            "Episode 3120\tLast reward: -4946.31\tAverage reward: -8042.56\n",
            "Episode 3130\tLast reward: -5707.13\tAverage reward: -8555.50\n",
            "Episode 3140\tLast reward: -11222.70\tAverage reward: -8356.26\n",
            "Episode 3150\tLast reward: -10293.41\tAverage reward: -7745.85\n",
            "Episode 3160\tLast reward: -5150.30\tAverage reward: -7741.27\n",
            "Episode 3170\tLast reward: -8696.95\tAverage reward: -8186.88\n",
            "Episode 3180\tLast reward: -5437.39\tAverage reward: -7498.16\n",
            "Episode 3190\tLast reward: -7436.60\tAverage reward: -7597.15\n",
            "Episode 3200\tLast reward: -5841.80\tAverage reward: -7518.21\n",
            "Episode 3210\tLast reward: -8596.73\tAverage reward: -8293.05\n",
            "Episode 3220\tLast reward: -5726.19\tAverage reward: -8033.76\n",
            "Episode 3230\tLast reward: -3492.87\tAverage reward: -8410.13\n",
            "Episode 3240\tLast reward: -10238.22\tAverage reward: -8374.18\n",
            "Episode 3250\tLast reward: -7787.56\tAverage reward: -7709.63\n",
            "Episode 3260\tLast reward: -9806.77\tAverage reward: -7972.33\n",
            "Episode 3270\tLast reward: -14915.06\tAverage reward: -8259.16\n",
            "Episode 3280\tLast reward: -4775.45\tAverage reward: -8052.00\n",
            "Episode 3290\tLast reward: -9933.64\tAverage reward: -8567.09\n",
            "Episode 3300\tLast reward: -11659.30\tAverage reward: -9146.02\n",
            "Episode 3310\tLast reward: -5401.67\tAverage reward: -9200.46\n",
            "Episode 3320\tLast reward: -6671.82\tAverage reward: -8455.31\n",
            "Episode 3330\tLast reward: -9557.78\tAverage reward: -9000.92\n",
            "Episode 3340\tLast reward: -5804.46\tAverage reward: -8460.37\n",
            "Episode 3350\tLast reward: -9371.20\tAverage reward: -8580.59\n",
            "Episode 3360\tLast reward: -9118.72\tAverage reward: -8392.93\n",
            "Episode 3370\tLast reward: -6506.85\tAverage reward: -8019.46\n",
            "Episode 3380\tLast reward: -2669.70\tAverage reward: -7764.34\n",
            "Episode 3390\tLast reward: -6423.46\tAverage reward: -7427.41\n",
            "Episode 3400\tLast reward: -7222.99\tAverage reward: -8651.93\n",
            "Episode 3410\tLast reward: -32066.56\tAverage reward: -10503.20\n",
            "Episode 3420\tLast reward: -7343.73\tAverage reward: -12025.61\n",
            "Episode 3430\tLast reward: -6771.27\tAverage reward: -13326.78\n",
            "Episode 3440\tLast reward: -6614.82\tAverage reward: -13759.48\n",
            "Episode 3450\tLast reward: -7484.29\tAverage reward: -12694.51\n",
            "Episode 3460\tLast reward: -25129.32\tAverage reward: -13215.49\n",
            "Episode 3470\tLast reward: -14836.64\tAverage reward: -14121.05\n",
            "Episode 3480\tLast reward: -7451.63\tAverage reward: -12544.08\n",
            "Episode 3490\tLast reward: -6452.40\tAverage reward: -11204.36\n",
            "Episode 3500\tLast reward: -7791.09\tAverage reward: -10397.41\n",
            "Episode 3510\tLast reward: -10483.87\tAverage reward: -12105.46\n",
            "Episode 3520\tLast reward: -15298.56\tAverage reward: -11122.38\n",
            "Episode 3530\tLast reward: -7077.61\tAverage reward: -9977.21\n",
            "Episode 3540\tLast reward: -6133.97\tAverage reward: -8625.95\n",
            "Episode 3550\tLast reward: -15588.49\tAverage reward: -8526.71\n",
            "Episode 3560\tLast reward: -9706.08\tAverage reward: -8452.86\n",
            "Episode 3570\tLast reward: -6013.08\tAverage reward: -7991.48\n",
            "Episode 3580\tLast reward: -4109.65\tAverage reward: -7313.09\n",
            "Episode 3590\tLast reward: -8834.67\tAverage reward: -7255.72\n",
            "Episode 3600\tLast reward: -7097.58\tAverage reward: -6634.26\n",
            "Episode 3610\tLast reward: -4699.14\tAverage reward: -6965.04\n",
            "Episode 3620\tLast reward: -11740.66\tAverage reward: -7608.21\n",
            "Episode 3630\tLast reward: -8846.61\tAverage reward: -7601.35\n",
            "Episode 3640\tLast reward: -13305.71\tAverage reward: -7907.10\n",
            "Episode 3650\tLast reward: -9851.62\tAverage reward: -8162.67\n",
            "Episode 3660\tLast reward: -6031.60\tAverage reward: -8586.08\n",
            "Episode 3670\tLast reward: -10323.70\tAverage reward: -9064.16\n",
            "Episode 3680\tLast reward: -5500.64\tAverage reward: -8591.44\n",
            "Episode 3690\tLast reward: -4278.81\tAverage reward: -8552.24\n",
            "Episode 3700\tLast reward: -8293.46\tAverage reward: -8701.79\n",
            "Episode 3710\tLast reward: -17496.56\tAverage reward: -8290.41\n",
            "Episode 3720\tLast reward: -5849.40\tAverage reward: -7795.98\n",
            "Episode 3730\tLast reward: -9174.50\tAverage reward: -7949.95\n",
            "Episode 3740\tLast reward: -15245.24\tAverage reward: -8957.95\n",
            "Episode 3750\tLast reward: -7380.89\tAverage reward: -8248.30\n",
            "Episode 3760\tLast reward: -9955.32\tAverage reward: -7661.38\n",
            "Episode 3770\tLast reward: -4224.76\tAverage reward: -8020.91\n",
            "Episode 3780\tLast reward: -9765.60\tAverage reward: -8510.07\n",
            "Episode 3790\tLast reward: -13213.40\tAverage reward: -8330.96\n",
            "Episode 3800\tLast reward: -4999.70\tAverage reward: -7920.01\n",
            "Episode 3810\tLast reward: -8381.18\tAverage reward: -8320.47\n",
            "Episode 3820\tLast reward: -8553.04\tAverage reward: -8687.57\n",
            "Episode 3830\tLast reward: -7689.28\tAverage reward: -8603.35\n",
            "Episode 3840\tLast reward: -5384.02\tAverage reward: -9366.58\n",
            "Episode 3850\tLast reward: -6600.54\tAverage reward: -9925.70\n",
            "Episode 3860\tLast reward: -7585.74\tAverage reward: -10188.62\n",
            "Episode 3870\tLast reward: -11991.33\tAverage reward: -10277.00\n",
            "Episode 3880\tLast reward: -9801.53\tAverage reward: -9572.53\n",
            "Episode 3890\tLast reward: -8196.66\tAverage reward: -9520.56\n",
            "Episode 3900\tLast reward: -8740.03\tAverage reward: -9792.67\n",
            "Episode 3910\tLast reward: -11160.56\tAverage reward: -9729.69\n",
            "Episode 3920\tLast reward: -6317.25\tAverage reward: -10006.62\n",
            "Episode 3930\tLast reward: -6552.32\tAverage reward: -10437.44\n",
            "Episode 3940\tLast reward: -18090.25\tAverage reward: -10732.84\n",
            "Episode 3950\tLast reward: -7597.22\tAverage reward: -10459.22\n",
            "Episode 3960\tLast reward: -9604.92\tAverage reward: -9925.70\n",
            "Episode 3970\tLast reward: -14131.92\tAverage reward: -10499.99\n",
            "Episode 3980\tLast reward: -9395.41\tAverage reward: -10542.25\n",
            "Episode 3990\tLast reward: -7914.36\tAverage reward: -10076.00\n",
            "Episode 4000\tLast reward: -9162.83\tAverage reward: -9566.77\n",
            "Episode 4010\tLast reward: -11456.82\tAverage reward: -10367.20\n",
            "Episode 4020\tLast reward: -13724.00\tAverage reward: -10113.13\n",
            "Episode 4030\tLast reward: -7012.78\tAverage reward: -9910.81\n",
            "Episode 4040\tLast reward: -8606.59\tAverage reward: -9571.04\n",
            "Episode 4050\tLast reward: -9073.16\tAverage reward: -8813.88\n",
            "Episode 4060\tLast reward: -11224.17\tAverage reward: -8694.73\n",
            "Episode 4070\tLast reward: -7412.57\tAverage reward: -8788.76\n",
            "Episode 4080\tLast reward: -4632.40\tAverage reward: -8040.06\n",
            "Episode 4090\tLast reward: -8404.66\tAverage reward: -8228.55\n",
            "Episode 4100\tLast reward: -5995.56\tAverage reward: -7660.43\n",
            "Episode 4110\tLast reward: -25065.65\tAverage reward: -8607.62\n",
            "Episode 4120\tLast reward: -8372.57\tAverage reward: -9642.11\n",
            "Episode 4130\tLast reward: -15047.13\tAverage reward: -10157.33\n",
            "Episode 4140\tLast reward: -3089.50\tAverage reward: -9250.83\n",
            "Episode 4150\tLast reward: -9111.78\tAverage reward: -11231.96\n",
            "Episode 4160\tLast reward: -8828.86\tAverage reward: -10595.26\n",
            "Episode 4170\tLast reward: -7374.19\tAverage reward: -10611.37\n",
            "Episode 4180\tLast reward: -16312.47\tAverage reward: -11167.97\n",
            "Episode 4190\tLast reward: -11885.59\tAverage reward: -10666.93\n",
            "Episode 4200\tLast reward: -8570.75\tAverage reward: -10966.27\n",
            "Episode 4210\tLast reward: -11954.18\tAverage reward: -10688.74\n",
            "Episode 4220\tLast reward: -9099.90\tAverage reward: -10398.81\n",
            "Episode 4230\tLast reward: -7724.71\tAverage reward: -10272.89\n",
            "Episode 4240\tLast reward: -10892.46\tAverage reward: -10626.29\n",
            "Episode 4250\tLast reward: -7700.76\tAverage reward: -10526.80\n",
            "Episode 4260\tLast reward: -10407.85\tAverage reward: -11426.71\n",
            "Episode 4270\tLast reward: -9518.88\tAverage reward: -10928.45\n",
            "Episode 4280\tLast reward: -11208.18\tAverage reward: -10915.56\n",
            "Episode 4290\tLast reward: -3092.92\tAverage reward: -10929.64\n",
            "Episode 4300\tLast reward: -6652.63\tAverage reward: -9816.71\n",
            "Episode 4310\tLast reward: -9582.04\tAverage reward: -10321.29\n",
            "Episode 4320\tLast reward: -6893.13\tAverage reward: -9525.77\n",
            "Episode 4330\tLast reward: -6948.82\tAverage reward: -9054.62\n",
            "Episode 4340\tLast reward: -15235.06\tAverage reward: -9497.02\n",
            "Episode 4350\tLast reward: -5066.46\tAverage reward: -10325.08\n",
            "Episode 4360\tLast reward: -22535.99\tAverage reward: -10505.17\n",
            "Episode 4370\tLast reward: -6017.53\tAverage reward: -10987.81\n",
            "Episode 4380\tLast reward: -10958.16\tAverage reward: -9944.89\n",
            "Episode 4390\tLast reward: -6682.44\tAverage reward: -10114.17\n",
            "Episode 4400\tLast reward: -7481.19\tAverage reward: -8602.51\n",
            "Episode 4410\tLast reward: -9625.77\tAverage reward: -9466.19\n",
            "Episode 4420\tLast reward: -8965.00\tAverage reward: -8793.90\n",
            "Episode 4430\tLast reward: -17300.41\tAverage reward: -9272.19\n",
            "Episode 4440\tLast reward: -15234.96\tAverage reward: -9498.64\n",
            "Episode 4450\tLast reward: -11571.41\tAverage reward: -9715.11\n",
            "Episode 4460\tLast reward: -14483.87\tAverage reward: -9806.20\n",
            "Episode 4470\tLast reward: -11427.90\tAverage reward: -10038.10\n",
            "Episode 4480\tLast reward: -12181.83\tAverage reward: -10474.29\n",
            "Episode 4490\tLast reward: -4164.82\tAverage reward: -9592.46\n",
            "Episode 4500\tLast reward: -7096.09\tAverage reward: -9685.11\n",
            "Episode 4510\tLast reward: -15485.63\tAverage reward: -9968.34\n",
            "Episode 4520\tLast reward: -2843.20\tAverage reward: -10078.19\n",
            "Episode 4530\tLast reward: -10894.36\tAverage reward: -9535.40\n",
            "Episode 4540\tLast reward: -14232.27\tAverage reward: -9800.94\n",
            "Episode 4550\tLast reward: -11703.90\tAverage reward: -9974.30\n",
            "Episode 4560\tLast reward: -8818.07\tAverage reward: -9798.03\n",
            "Episode 4570\tLast reward: -11057.47\tAverage reward: -10492.54\n",
            "Episode 4580\tLast reward: -13155.32\tAverage reward: -10903.60\n",
            "Episode 4590\tLast reward: -9972.92\tAverage reward: -10127.79\n",
            "Episode 4600\tLast reward: -11347.62\tAverage reward: -9815.93\n",
            "Episode 4610\tLast reward: -6732.82\tAverage reward: -9711.92\n",
            "Episode 4620\tLast reward: -6973.92\tAverage reward: -9042.62\n",
            "Episode 4630\tLast reward: -6355.00\tAverage reward: -8142.84\n",
            "Episode 4640\tLast reward: -8521.52\tAverage reward: -8189.97\n",
            "Episode 4650\tLast reward: -6847.60\tAverage reward: -8351.98\n",
            "Episode 4660\tLast reward: -11707.43\tAverage reward: -8680.92\n",
            "Episode 4670\tLast reward: -7256.82\tAverage reward: -8274.32\n",
            "Episode 4680\tLast reward: -10311.05\tAverage reward: -8137.20\n",
            "Episode 4690\tLast reward: -12759.48\tAverage reward: -8956.23\n",
            "Episode 4700\tLast reward: -14484.52\tAverage reward: -8913.29\n",
            "Episode 4710\tLast reward: -5852.23\tAverage reward: -8193.41\n",
            "Episode 4720\tLast reward: -5196.13\tAverage reward: -7811.38\n",
            "Episode 4730\tLast reward: -9320.67\tAverage reward: -8817.76\n",
            "Episode 4740\tLast reward: -6521.76\tAverage reward: -8276.98\n",
            "Episode 4750\tLast reward: -11073.03\tAverage reward: -8532.98\n",
            "Episode 4760\tLast reward: -10456.13\tAverage reward: -8560.16\n",
            "Episode 4770\tLast reward: -7091.87\tAverage reward: -8333.80\n",
            "Episode 4780\tLast reward: -7799.41\tAverage reward: -7817.89\n",
            "Episode 4790\tLast reward: -14779.65\tAverage reward: -7551.39\n",
            "Episode 4800\tLast reward: -8027.74\tAverage reward: -7746.03\n",
            "Episode 4810\tLast reward: -6297.23\tAverage reward: -10446.08\n",
            "Episode 4820\tLast reward: -8225.32\tAverage reward: -9620.91\n",
            "Episode 4830\tLast reward: -7440.70\tAverage reward: -9520.25\n",
            "Episode 4840\tLast reward: -9237.63\tAverage reward: -9126.90\n",
            "Episode 4850\tLast reward: -10872.32\tAverage reward: -9164.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee5VGcwjMxQ6",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-rh5ZhpHHw",
        "colab_type": "text"
      },
      "source": [
        "We did many things\n",
        "Nothing worked\n",
        "What we learned: RL is BS\n",
        "LAME\n",
        "Just put a PID controller on your vehicles, and you'll be fine..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eau97K6nND6p",
        "colab_type": "code",
        "outputId": "3cf452f8-e716-430f-8c55-1621f47e051b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "!unzip quad_sim.zip -d ./"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  quad_sim.zip\n",
            "  inflating: ./quad_sim/config.py    \n",
            "  inflating: ./quad_sim/ctrl.py      \n",
            "   creating: ./quad_sim/quadFiles/\n",
            " extracting: ./quad_sim/quadFiles/__init__.py  \n",
            "  inflating: ./quad_sim/quadFiles/initQuad.py  \n",
            "  inflating: ./quad_sim/quadFiles/quad.py  \n",
            "  inflating: ./quad_sim/run_3D_simulation.py  \n",
            "  inflating: ./quad_sim/trajectory.py  \n",
            "   creating: ./quad_sim/utils/\n",
            "  inflating: ./quad_sim/utils/__init__.py  \n",
            "  inflating: ./quad_sim/utils/animation.py  \n",
            "  inflating: ./quad_sim/utils/display.py  \n",
            "  inflating: ./quad_sim/utils/mixer.py  \n",
            "  inflating: ./quad_sim/utils/quaternionFunctions.py  \n",
            "  inflating: ./quad_sim/utils/rotationConversion.py  \n",
            "  inflating: ./quad_sim/utils/stateConversions.py  \n",
            "  inflating: ./quad_sim/utils/windModel.py  \n",
            "  inflating: ./quad_sim/waypoints.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AILjVB0NL6F",
        "colab_type": "code",
        "outputId": "9adb618d-4dd4-4aea-ccf6-1c7a53208931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!pip install gym\n",
        "!pip install Box2D gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: Box2D in /usr/local/lib/python3.6/dist-packages (2.3.10)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhnXhOFWNXIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}